<!doctype html><html lang=en class=no-js> <head><link rel=stylesheet href=assets/javascripts/main.bec4e691.min.css><link rel=stylesheet href=assets/javascripts/palette.96b5c56e.min.css><link rel=stylesheet href=assets/javascripts/main.9e77ca7e.min.css><link rel=stylesheet href=assets/javascripts/home.def3a648.min.css><link rel=stylesheet href=assets/javascripts/css><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=author content="Yichu Yang"><link href=https://yichuyang.com/notes/RL/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%B0%8F%E6%8A%84/ rel=canonical><link rel=icon href=../../assets/favicon.png><meta name=generator content="mkdocs-1.3.0, mkdocs-material-8.2.13"><title>强化学习小抄 - Yichu's blog</title><link rel=stylesheet href=../../assets/stylesheets/main.e411adfe.min.css><link rel=stylesheet href=../../assets/stylesheets/palette.cc9b2e1e.min.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><link rel=stylesheet href=../../stylesheets/extra.css><script>__md_scope=new URL("../..",location),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo> <script>var palette=__md_get("__palette");if(palette&&"object"==typeof palette.color)for(var key of Object.keys(palette.color))document.body.setAttribute("data-md-color-"+key,palette.color[key])</script> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#_1 class=md-skip> Skip to content </a> </div> <div data-md-component=announce> </div> <header class="md-header md-header--lifted" data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=../.. title="Yichu's blog" class="md-header__button md-logo" aria-label="Yichu's blog" data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 89 89"> <path d=M3.136,17.387l0,42.932l42.932,21.467l-42.932,-64.399Z /> <path d=M21.91,8l42.933,64.398l-18.775,9.388l-42.932,-64.399l18.774,-9.387Z style="fill-opacity: 0.5"/> <path d=M67.535,17.387l-27.262,18.156l21.878,32.818l5.384,2.691l0,-53.665Z /> <path d=M67.535,17.387l0,53.666l18.774,-9.388l0,-53.665l-18.774,9.387Z style="fill-opacity: 0.25"/> </svg> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> Yichu's blog </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> 强化学习小抄 </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo aria-label="Switch to dark mode" type=radio name=__palette id=__palette_1> <label class="md-header__button md-icon" title="Switch to dark mode" for=__palette_2 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5m0 8a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3Z"/></svg> </label> <input class=md-option data-md-color-media data-md-color-scheme=slate data-md-color-primary=white data-md-color-accent=cyan aria-label="Switch to light mode" type=radio name=__palette id=__palette_2> <label class="md-header__button md-icon" title="Switch to light mode" for=__palette_1 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M17 6H7c-3.31 0-6 2.69-6 6s2.69 6 6 6h10c3.31 0 6-2.69 6-6s-2.69-6-6-6zm0 10H7c-2.21 0-4-1.79-4-4s1.79-4 4-4h10c2.21 0 4 1.79 4 4s-1.79 4-4 4zM7 9c-1.66 0-3 1.34-3 3s1.34 3 3 3 3-1.34 3-3-1.34-3-3-3z"/></svg> </label> </form> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg> </label> <nav class=md-search__options aria-label=Search> <a href=javascript:void(0) class="md-search__icon md-icon" aria-label=Share data-clipboard data-clipboard-text data-md-component=search-share tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7 0-.24-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91 1.61 0 2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08Z"/></svg> </a> <button type=reset class="md-search__icon md-icon" aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list></ol> </div> </div> </div> </div> </div> </nav> <nav class=md-tabs aria-label=Tabs data-md-component=tabs> <div class="md-tabs__inner md-grid"> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=../.. class=md-tabs__link> Home </a> </li> <li class=md-tabs__item> <a href=../../Sensor-Fusion/LOAM/ class=md-tabs__link> Sensor Fusion </a> </li> <li class=md-tabs__item> <a href=../../Motion-Planning/BCH-approximation/ class=md-tabs__link> Motion Planning </a> </li> <li class=md-tabs__item> <a href=../%E6%89%8B%E6%92%95%E5%B8%B8%E7%94%A8%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E4%BB%A3%E7%A0%81/ class=md-tabs__link> 强化学习 </a> </li> <li class=md-tabs__item> <a href=https://yichuyang.com class=md-tabs__link> Resume </a> </li> </ul> </div> </nav> </header> <div class=md-container data-md-component=container> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=../.. title="Yichu's blog" class="md-nav__button md-logo" aria-label="Yichu's blog" data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 89 89"> <path d=M3.136,17.387l0,42.932l42.932,21.467l-42.932,-64.399Z /> <path d=M21.91,8l42.933,64.398l-18.775,9.388l-42.932,-64.399l18.774,-9.387Z style="fill-opacity: 0.5"/> <path d=M67.535,17.387l-27.262,18.156l21.878,32.818l5.384,2.691l0,-53.665Z /> <path d=M67.535,17.387l0,53.666l18.774,-9.388l0,-53.665l-18.774,9.387Z style="fill-opacity: 0.25"/> </svg> </a> Yichu's blog </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../.. class=md-nav__link> Home </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_2 data-md-state=indeterminate type=checkbox id=__nav_2 checked> <label class=md-nav__link for=__nav_2> Sensor Fusion <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Sensor Fusion" data-md-level=1> <label class=md-nav__title for=__nav_2> <span class="md-nav__icon md-icon"></span> Sensor Fusion </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../Sensor-Fusion/LOAM/ class=md-nav__link> LOAM </a> </li> <li class=md-nav__item> <a href=../../Sensor-Fusion/Ceres-Tutorial/ class=md-nav__link> Ceres 使用入门 </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_2_3 data-md-state=indeterminate type=checkbox id=__nav_2_3 checked> <label class=md-nav__link for=__nav_2_3> 惯性导航 <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=惯性导航 data-md-level=2> <label class=md-nav__title for=__nav_2_3> <span class="md-nav__icon md-icon"></span> 惯性导航 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../Sensor-Fusion/%E5%9B%9B%E5%85%83%E6%95%B0%E5%9F%BA%E7%A1%80%E5%92%8C%E6%83%AF%E6%80%A7%E5%AF%BC%E8%88%AA%E8%A7%A3%E7%AE%97/ class=md-nav__link> 四元数基础和惯性导航解算 </a> </li> <li class=md-nav__item> <a href=../../Sensor-Fusion/%E6%83%AF%E6%80%A7%E5%AF%BC%E8%88%AA%E5%8E%9F%E7%90%86%E5%8F%8A%E8%AF%AF%E5%B7%AE%E5%88%86%E6%9E%90/ class=md-nav__link> 惯性导航原理及误差分析 </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_2_4 data-md-state=indeterminate type=checkbox id=__nav_2_4 checked> <label class=md-nav__link for=__nav_2_4> 卡尔曼滤波 <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=卡尔曼滤波 data-md-level=2> <label class=md-nav__title for=__nav_2_4> <span class="md-nav__icon md-icon"></span> 卡尔曼滤波 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../Sensor-Fusion/%E5%8D%A1%E5%B0%94%E6%9B%BC%E6%BB%A4%E6%B3%A2/ class=md-nav__link> 卡尔曼滤波 </a> </li> <li class=md-nav__item> <a href=../../Sensor-Fusion/%E5%8D%A1%E5%B0%94%E6%9B%BC%E6%BB%A4%E6%B3%A2%E6%8E%A8%E5%AF%BC/ class=md-nav__link> 卡尔曼滤波推导 </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_2_5 data-md-state=indeterminate type=checkbox id=__nav_2_5 checked> <label class=md-nav__link for=__nav_2_5> 误差状态卡尔曼(ESKF)笔记 <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=误差状态卡尔曼(ESKF)笔记 data-md-level=2> <label class=md-nav__title for=__nav_2_5> <span class="md-nav__icon md-icon"></span> 误差状态卡尔曼(ESKF)笔记 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../Sensor-Fusion/ESKF%E5%8A%A8%E5%8A%9B%E5%AD%A6%E6%96%B9%E7%A8%8B/ class=md-nav__link> ESKF动力学方程 </a> </li> <li class=md-nav__item> <a href=../../Sensor-Fusion/ESKF%E7%A6%BB%E6%95%A3%E5%8A%A8%E5%8A%9B%E5%AD%A6%E6%96%B9%E7%A8%8B/ class=md-nav__link> ESKF离散动力学方程 </a> </li> <li class=md-nav__item> <a href=../../Sensor-Fusion/ESKF%E8%AF%AF%E5%B7%AE%E7%8A%B6%E6%80%81%E6%BB%A4%E6%B3%A2%E5%99%A8/ class=md-nav__link> ESKF误差状态滤波器 </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_2_6 data-md-state=indeterminate type=checkbox id=__nav_2_6 checked> <label class=md-nav__link for=__nav_2_6> 图优化 <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=图优化 data-md-level=2> <label class=md-nav__title for=__nav_2_6> <span class="md-nav__icon md-icon"></span> 图优化 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../Sensor-Fusion/%E9%A2%84%E7%A7%AF%E5%88%86%E6%8E%A8%E5%AF%BC/ class=md-nav__link> 预积分推导 </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_3 data-md-state=indeterminate type=checkbox id=__nav_3 checked> <label class=md-nav__link for=__nav_3> Motion Planning <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Motion Planning" data-md-level=1> <label class=md-nav__title for=__nav_3> <span class="md-nav__icon md-icon"></span> Motion Planning </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../Motion-Planning/BCH-approximation/ class=md-nav__link> BCH approximation </a> </li> <li class=md-nav__item> <a href=../../Motion-Planning/ICP-realization/ class=md-nav__link> ICP realization </a> </li> <li class=md-nav__item> <a href=../../Motion-Planning/matrix_diff/ class=md-nav__link> Matrix diff </a> </li> <li class=md-nav__item> <a href=../../Motion-Planning/trajectory-optimization/ class=md-nav__link> Trajectory optimization </a> </li> <li class=md-nav__item> <a href=../../Motion-Planning/Polynomial-Trajectory-Planning-for-Aggressive-Quadrotor-Flight/ class=md-nav__link> Polynomial Trajectory Planning for Aggressive Quadrotor Flight </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_3_6 data-md-state=indeterminate type=checkbox id=__nav_3_6 checked> <label class=md-nav__link for=__nav_3_6> Fast/Ego-Planner <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=Fast/Ego-Planner data-md-level=2> <label class=md-nav__title for=__nav_3_6> <span class="md-nav__icon md-icon"></span> Fast/Ego-Planner </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_3_6_1 data-md-state=indeterminate type=checkbox id=__nav_3_6_1 checked> <label class=md-nav__link for=__nav_3_6_1> 地图形式 <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=地图形式 data-md-level=3> <label class=md-nav__title for=__nav_3_6_1> <span class="md-nav__icon md-icon"></span> 地图形式 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../Motion-Planning/Occupancy-Grid-Map/ class=md-nav__link> Occupancy Grid Map </a> </li> <li class=md-nav__item> <a href=../../Motion-Planning/ESDF/ class=md-nav__link> ESDF </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../../Motion-Planning/B-spline/ class=md-nav__link> B spline </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_4 data-md-state=indeterminate type=checkbox id=__nav_4 checked> <label class=md-nav__link for=__nav_4> 强化学习 <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=强化学习 data-md-level=1> <label class=md-nav__title for=__nav_4> <span class="md-nav__icon md-icon"></span> 强化学习 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../%E6%89%8B%E6%92%95%E5%B8%B8%E7%94%A8%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E4%BB%A3%E7%A0%81/ class=md-nav__link> 手撕常用强化学习代码 </a> </li> <li class=md-nav__item> <a href=../DRQN%E5%AE%9E%E7%8E%B0%E7%9A%84%E4%B8%80%E4%BA%9B%E7%BB%86%E8%8A%82/ class=md-nav__link> DRQN实现的一些细节 </a> </li> <li class=md-nav__item> <a href=../R2D2-%E7%AC%94%E8%AE%B0/ class=md-nav__link> R2D2-笔记 </a> </li> <li class=md-nav__item> <a href=../Diffusion-Model/ class=md-nav__link> Diffusion Model </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_5 data-md-state=indeterminate type=checkbox id=__nav_5 checked> <label class=md-nav__link for=__nav_5> Resume <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=Resume data-md-level=1> <label class=md-nav__title for=__nav_5> <span class="md-nav__icon md-icon"></span> Resume </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=https://yichuyang.com class=md-nav__link> None </a> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#_1 class=md-nav__link> 基础知识 </a> <nav class=md-nav aria-label=基础知识> <ul class=md-nav__list> <li class=md-nav__item> <a href=#_2 class=md-nav__link> 马尔可夫决策过程 </a> </li> <li class=md-nav__item> <a href=#_3 class=md-nav__link> 目标 </a> </li> <li class=md-nav__item> <a href=#policy class=md-nav__link> 策略 Policy </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#solving-finite-state-mdps class=md-nav__link> Solving finite-state MDPs </a> </li> <li class=md-nav__item> <a href=#td class=md-nav__link> TD </a> </li> <li class=md-nav__item> <a href=#function-approximation class=md-nav__link> Function approximation </a> <nav class=md-nav aria-label="Function approximation"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#incremental-methods class=md-nav__link> Incremental methods </a> </li> <li class=md-nav__item> <a href=#batch-methods class=md-nav__link> Batch methods </a> </li> <li class=md-nav__item> <a href=#least-squares class=md-nav__link> Least squares </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#policy-gradients class=md-nav__link> Policy Gradients </a> <nav class=md-nav aria-label="Policy Gradients"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#model-free-vs-model-based-rl class=md-nav__link> Model-Free vs Model-Based RL </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#policy-gradient class=md-nav__link> Policy Gradient </a> <nav class=md-nav aria-label="Policy Gradient"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#simple-pg-example class=md-nav__link> Simple PG example: </a> </li> <li class=md-nav__item> <a href=#expected-grad-log-prob-eglp-lemma class=md-nav__link> Expected Grad-Log-Prob (EGLP) lemma </a> </li> <li class=md-nav__item> <a href=#baseline class=md-nav__link> Baseline </a> </li> <li class=md-nav__item> <a href=#algorithms-see class=md-nav__link> Algorithms: see 手撕强化学习代码 </a> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <nav class=md-tags> <a href=../../tags/#notes class=md-tag> 强化学习，Notes </a> </nav> <h1>强化学习小抄</h1> <p>参考：<a href=https://www.cs.jhu.edu/~paxia/notes/rl_notes.html>https://www.cs.jhu.edu/~paxia/notes/rl_notes.html</a></p> <p>Value Function <span class=arithmatex>\(V_π(s)=E[R(s_0)+γR(s_1)+γ^2R(s_2)...|s_0=s,π]\)</span></p> <p>Bellman equation:<span class=arithmatex>\(V_\pi(s)=R(s)+\gamma\sum_{s'\in S}P_{s\pi(a)}(s')V_\pi(s')\)</span></p> <h2 id=_1>基础知识<a class=headerlink href=#_1 title="Permanent link">¶</a></h2> <h3 id=_2>马尔可夫决策过程<a class=headerlink href=#_2 title="Permanent link">¶</a></h3> <p>马尔可夫决策过程可以表示成一个tuple:</p> <div class=arithmatex>\[ (S,A,P_{sa},\gamma,R) \]</div> <p>S为set of states，A is a set of actions, Psa are transition probabilities, γ is a discount factor and R:S×A→R is the reward function.</p> <h3 id=_3>目标<a class=headerlink href=#_3 title="Permanent link">¶</a></h3> <p>目标是最大化 the expected reward： <span class=arithmatex>\(E[R(s0)+γR(s1)+γ^2R(s2)...]\)</span> </p> <h3 id=policy>策略 Policy<a class=headerlink href=#policy title="Permanent link">¶</a></h3> <p>策略是一个函数 π:S→A, 即由某一状态导出动作： <span class=arithmatex>\(ai=π(si)\)</span>. </p> <p>一个策略可以对应一个价值函数 Value Function：</p> <div class=arithmatex>\[ V^π(s)=E[R(s0)+γR(s1)+γ^2R(s2)...|s0,π] \]</div> <p>对于一个特定策略，其价值函数应该满足贝尔曼方程Bellman equation： </p> <div class=arithmatex>\[ V^π(s)=R(s)+γ\sum_{s'∈S}P_{sπ}(s)(s')V^π(s') \]</div> <p>即由转移概率可以到达的所有状态的价值的加权平均。R(s) is the immediate reward。 </p> <p>每个状态对应的最优策略： <span class=arithmatex>\(V^∗(s)=max_πV^π(s)\)</span> ，对应的最优价值函数： </p> <div class=arithmatex>\[ V^∗(s)=R(s)+\max_aγ \sum_{s'∈S}P_{sa}(s')V^∗(s'). \]</div> <h2 id=solving-finite-state-mdps>Solving finite-state MDPs<a class=headerlink href=#solving-finite-state-mdps title="Permanent link">¶</a></h2> <table> <thead> <tr> <th></th> <th></th> <th></th> </tr> </thead> <tbody> <tr> <td>Value Iteration</td> <td></td> <td><span class=arithmatex>\(V(s)=R(s)+max_aγ∑_{s'∈S}P_{sa}(s')V(s')\)</span></td> </tr> <tr> <td>Policy Iteration</td> <td></td> <td>π(s)=argmaxa∑s'Psa(s')V(s')\pi(s) = \arg\max_a \sum_{s'} P_sa(s')V(s')</td> </tr> </tbody> </table> <p>蒙特卡洛方法和TD方法区别：因为蒙特卡洛方法进行完整的采样来获取了长期的回报值，因而在价值估计上会有着更小的偏差，但是也正因为收集了完整的信息 所以价值的方差会更大「毕竟基于试验的采样得到 和真实的分布还是有差距 不充足的交互导致的较大方差」；而TD算法与其相反 因为只考虑了前一步的回报值 其他都是基于之前的估计值 因而估计具有偏差 但方差也较小；</p> <h2 id=td>TD<a class=headerlink href=#td title="Permanent link">¶</a></h2> <table> <thead> <tr> <th></th> <th></th> <th></th> <th></th> </tr> </thead> <tbody> <tr> <td>MC</td> <td>TD(1) or <span class=arithmatex>\(\infty\)</span>-step TD</td> <td>unbiased</td> <td>Markov/non Markov</td> </tr> <tr> <td>TD</td> <td>TD(0) or 1-step TD</td> <td>biased</td> <td>Markov environment</td> </tr> <tr> <td>TD(<span class=arithmatex>\(\lambda\)</span>)</td> <td><span class=arithmatex>\(G_{t}^\lambda = (1-\lambda)\sum_{n=1}^{T-t-1} \lambda^{n-1}G_{t:t+n}+\lambda^{T-t-n}G_t\)</span></td> <td></td> <td></td> </tr> </tbody> </table> <!--more--> <p>Simplified Bellman Equation: <span class=arithmatex>\(Q_\pi(s_t,a_t)=E(R(s)+\gamma Q_\pi(s_{t+1},a_{t+1}))\)</span></p> <p>Temperal Difference: V(s)←V(s)+α(Y−V(s))</p> <table> <thead> <tr> <th></th> <th></th> <th></th> </tr> </thead> <tbody> <tr> <td>SARSA</td> <td>On-policy</td> <td><span class=arithmatex>\(Q_π(s_t,a_t)←Q_π(s_t,a_t)+α[r_t+γQ_π(s_{t+1},a_{t+1})−Q_π(s_t,a_t)]\)</span>, <span class=arithmatex>\(a_{t+1}=π(s_{t+1})\)</span></td> </tr> <tr> <td>Q-Learning</td> <td>Off-policy</td> <td><span class=arithmatex>\(Q(s_t,a_t)←Q(s_t,a_t)+α[r_t+γmax_{a_{t+1}}(Q(s_{t+1},a_{t+1})−Q(s_t,a_t))]\)</span></td> </tr> </tbody> </table> <h2 id=function-approximation>Function approximation<a class=headerlink href=#function-approximation title="Permanent link">¶</a></h2> <p>状态空间太大的时候，用函数逼近</p> <h3 id=incremental-methods>Incremental methods<a class=headerlink href=#incremental-methods title="Permanent link">¶</a></h3> <p>定义损失函数：<span class=arithmatex>\(J(w) = E_{\pi}[(V^{\pi}(s) - \hat{V}(s, w))^2]\)</span> GD/SGD to update: <span class=arithmatex>\(<span class=arithmatex>\(\delta w = -\frac{1}{2} \nabla_wJ(w)\)</span>\)</span> <span class=arithmatex>\(<span class=arithmatex>\(\delta w = \alpha(V^{\pi}(s) - \hat{V}(s, w))\nabla_w\hat{V}(s, w)\)</span>\)</span></p> <h3 id=batch-methods>Batch methods<a class=headerlink href=#batch-methods title="Permanent link">¶</a></h3> <div class=arithmatex>\[L_i(w_i) = E_{(s, a, r, s') \sim \mathcal{D}}\left[\left(r + \gamma \max_{a'} Q(s' ,a ; w_{i}^{\text{old}}) - Q(s, a; w_{i}\right)^2\right]\]</div> <h3 id=least-squares>Least squares<a class=headerlink href=#least-squares title="Permanent link">¶</a></h3> <p><span class=arithmatex>\(\hat{V}(s,w) = x(s)^\top w\)</span></p> <h2 id=policy-gradients>Policy Gradients<a class=headerlink href=#policy-gradients title="Permanent link">¶</a></h2> <p>Choose An Objective Funciton:<span class=arithmatex>\(<span class=arithmatex>\(J_1(\theta) = V^{\pi_\theta}(s_1) = E_{\pi_0}[V_1]\)</span>\)</span> <span class=arithmatex>\(<span class=arithmatex>\(J_{avV}(\theta) = \sum_s d^{\pi_\theta}(s)V^{\pi_\theta}(s)\)</span>\)</span> <span class=arithmatex>\(<span class=arithmatex>\(J_{avR}(\theta) = \sum_{s} d^{\pi_\theta}(s)\sum_a \pi_{\theta}(s, a)R_s^a\)</span>\)</span> Update using policy gradient: <span class=arithmatex>\(<span class=arithmatex>\(\delta \theta =\alpha\nabla_\theta J(\theta)\)</span>\)</span> <span class=arithmatex>\(<span class=arithmatex>\(\nabla_\theta J(\theta) = E_{\pi_\theta}[\nabla_{\theta} \log \pi_\theta(s,a) Q^{\pi_\theta}(s, a)]\)</span>\)</span></p> <table> <thead> <tr> <th></th> <th></th> </tr> </thead> <tbody> <tr> <td>REINFORCE</td> <td>MC policy gradient，随机初始化<span class=arithmatex>\(\theta\)</span>,更新：<span class=arithmatex>\(\theta \rightarrow \theta + \alpha \nabla_{\theta}\log\pi_{\theta}(s_t, a_t)v_t\)</span></td> </tr> <tr> <td>Actor-Critic</td> <td>approximate both value and policy，<span class=arithmatex>\(\delta \theta = \alpha\nabla_\theta \log \pi_\theta(s, a)Q_w(s,a)\)</span>, update both <span class=arithmatex>\(\theta\)</span> and ww with each step</td> </tr> <tr> <td></td> <td>使用baseline减少方差，如果baseline就是<span class=arithmatex>\(V^{\pi_\theta}(s)\)</span>,上式变成<span class=arithmatex>\(E_{\pi_\theta}[\nabla_\theta \log \pi_\theta(s, a) (Q^{\pi_\theta}(s, a) - B(s))] = E_{\pi_\theta}[\nabla_\theta \log \pi_\theta(s, a) (Q^{\pi_\theta}(s, a) - V^{\pi_\theta}(s))]\)</span>，此处Q-V即Advantage Function，也可以直接用函数逼近这个Advantage。</td> </tr> </tbody> </table> <h3 id=model-free-vs-model-based-rl>Model-Free vs Model-Based RL<a class=headerlink href=#model-free-vs-model-based-rl title="Permanent link">¶</a></h3> <p>The main upside to having a model is that <strong>it allows the agent to plan</strong> by thinking ahead.</p> <p>The main downside is that **a ground-truth model of the environment is usually not available to the agent. </p> <p><strong>Policy Optimization</strong>:They optimize the parameters <img alt=\theta src=https://spinningup.openai.com/en/latest/_images/math/ce5edddd490112350f4bd555d9390e0e845f754a.svg> either directly by gradient ascent on the performance objective <img alt=J(\pi_{\theta}) src=https://spinningup.openai.com/en/latest/_images/math/96b876944de9cf0f980fe261562e8e07029245bf.svg>, or indirectly, by maximizing local approximations of <img alt=J(\pi_{\theta}) src=https://spinningup.openai.com/en/latest/_images/math/96b876944de9cf0f980fe261562e8e07029245bf.svg>. This optimization is almost always performed <strong>on-policy</strong>, which means that each update only uses data collected while acting according to the most recent version of the policy. Policy optimization also usually involves learning an approximator <img alt=V_{\phi}(s) src=https://spinningup.openai.com/en/latest/_images/math/693bb706835fbd5903ad9758837acecd07ef13b1.svg> for the on-policy value function <img alt=V^{\pi}(s) src=https://spinningup.openai.com/en/latest/_images/math/a81303323c25fc13cd0652ca46d7596276e5cb7e.svg>, which gets used in figuring out how to update the policy.</p> <p><strong>Q-Learning.</strong> Methods in this family learn an approximator <img alt=Q_{\theta}(s,a) src=https://spinningup.openai.com/en/latest/_images/math/de947d14fdcfaa155ef3301fc39efcf9e6c9449c.svg> for the optimal action-value function, <img alt=Q^*(s,a) src=https://spinningup.openai.com/en/latest/_images/math/cbed396f671d6fb54f6df5c044b82ab3f052d63e.svg>This optimization is almost always performed <strong>off-policy</strong>, which means that each update can use data collected at any point during training</p> <p><strong>Trade-offs Between Policy Optimization and Q-Learning</strong></p> <p>PG more stable, Q-learning more data efficent.</p> <h2 id=policy-gradient>Policy Gradient<a class=headerlink href=#policy-gradient title="Permanent link">¶</a></h2> <h3 id=simple-pg-example>Simple PG example:<a class=headerlink href=#simple-pg-example title="Permanent link">¶</a></h3> <p>expected return<img alt="J(\pi_{\theta}) = \underE{\tau \sim \pi_{\theta}}{R(\tau)}" src=https://spinningup.openai.com/en/latest/_images/math/42bfa236d63e32e501baf89345748061540e102d.svg></p> <p>optimize the policy by gradient ascent<img alt="\theta_{k+1} = \theta_k + \alpha \left. \nabla_{\theta} J(\pi_{\theta}) \right|_{\theta_k}." src=https://spinningup.openai.com/en/latest/_images/math/595de3acc68fed2ec24c11420d9abbee013497ac.svg><img alt="\begin{align*} \nabla_{\theta} J(\pi_{\theta}) &= \nabla_{\theta} \underE{\tau \sim \pi_{\theta}}{R(\tau)} & \ &= \nabla_{\theta} \int_{\tau} P(\tau|\theta) R(\tau) & \text{Expand expectation} \ &= \int_{\tau} \nabla_{\theta} P(\tau|\theta) R(\tau) & \text{Bring gradient under integral} \ &= \int_{\tau} P(\tau|\theta) \nabla_{\theta} \log P(\tau|\theta) R(\tau) & \text{Log-derivative trick} \ &= \underE{\tau \sim \pi_{\theta}}{\nabla_{\theta} \log P(\tau|\theta) R(\tau)} & \text{Return to expectation form} \ \therefore \nabla_{\theta} J(\pi_{\theta}) &= \underE{\tau \sim \pi_{\theta}}{\sum_{t=0}^{T} \nabla_{\theta} \log \pi_{\theta}(a_t |s_t) R(\tau)} & \text{Expression for grad-log-prob} \end{align*}" src=https://spinningup.openai.com/en/latest/_images/math/b5e135d2ae389147267372abc1c5b20e644ec881.svg></p> <p>其中第五步:<img alt="\nabla_{\theta} \log P(\tau | \theta) &= \cancel{\nabla_{\theta} \log \rho_0 (s_0)} + \sum_{t=0}^{T} \bigg( \cancel{\nabla_{\theta} \log P(s_{t+1}|s_t, a_t)}  + \nabla_{\theta} \log \pi_{\theta}(a_t |s_t)\bigg) \ &= \sum_{t=0}^{T} \nabla_{\theta} \log \pi_{\theta}(a_t |s_t)." src=https://spinningup.openai.com/en/latest/_images/math/3ef66d94ee26cfa69015915dbd112ea78fb5e7ba.svg></p> <h3 id=expected-grad-log-prob-eglp-lemma>Expected Grad-Log-Prob (EGLP) lemma<a class=headerlink href=#expected-grad-log-prob-eglp-lemma title="Permanent link">¶</a></h3> <p><strong>EGLP Lemma.</strong> Suppose that <img alt=P_{\theta} src=https://spinningup.openai.com/en/latest/_images/math/9c44674c334586fd6d417281ea223857ea3ee0d6.svg> is a parameterized probability distribution over a random variable, <img alt=x src=https://spinningup.openai.com/en/latest/_images/math/ea07a4204f1f53321f76d9c7e348199f0d707db1.svg>. Then:</p> <p><img alt="\underE{x \sim P_{\theta}}{\nabla_{\theta} \log P_{\theta}(x)} = 0." src=https://spinningup.openai.com/en/latest/_images/math/f7a02965e7c07537dfb98391da78ab7613e887f7.svg></p> <h3 id=baseline>Baseline<a class=headerlink href=#baseline title="Permanent link">¶</a></h3> <p><img alt="\nabla_{\theta} J(\pi_{\theta}) = \underE{\tau \sim \pi_{\theta}}{\sum_{t=0}^{T} \nabla_{\theta} \log \pi_{\theta}(a_t |s_t) \left(\sum_{t'=t}^T R(s_{t'}, a_{t'}, s_{t'+1}) - b(s_t)\right)}." src=https://spinningup.openai.com/en/latest/_images/math/3a111dcb6e04aa632bd69e9a7e769e06e2530a0a.svg></p> <p>Any function <img alt=b src=https://spinningup.openai.com/en/latest/_images/math/99ac829ad51642abad0797da299214e7ce1da722.svg> used in this way is called a <strong>baseline</strong>. The most common choice of baseline is the <a href=https://spinningup.openai.com/en/latest/spinningup/rl_intro.html#value-functions>on-policy value function</a> <img alt=V^{\pi}(s_t) src=https://spinningup.openai.com/en/latest/_images/math/263857fcb5eaaf46a4e9a5e6ce2be414d95748aa.svg></p> <p><img alt=V^{\pi}(s_t) src=https://spinningup.openai.com/en/latest/_images/math/263857fcb5eaaf46a4e9a5e6ce2be414d95748aa.svg> cannot be computed exactly is usually approximated with a neural network, <img alt=V_{\phi}(s_t) src=https://spinningup.openai.com/en/latest/_images/math/d313ca98e5fb043c581841a09ea19bce2ce53b04.svg>, then the goal is:</p> <p><img alt="\phi_k = \arg \min_{\phi} \underE{s_t, \hat{R}_t \sim \pi_k}{\left( V_{\phi}(s_t) - \hat{R}_t \right)^2}," src=https://spinningup.openai.com/en/latest/_images/math/a82208dd637243514710948c4ebbc3c59e9a2e57.svg></p> <h3 id=algorithms-see>Algorithms: see <a href=https://yichuyang.com/blog/2021/11/27/%E6%89%8B%E6%92%95%E5%B8%B8%E7%94%A8%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E4%BB%A3%E7%A0%81/ >手撕强化学习代码</a><a class=headerlink href=#algorithms-see title="Permanent link">¶</a></h3> <p>在《RLbook2018》的11.3节中sutton提到，如果涉及到以下三个条件，那么基于值函数的迭代方法就不确保收敛，sutton将其称为”致命三要素” <strong>(deadly triad)</strong>:</p> <ol> <li>Function Approximation 函数近似。</li> <li>Bootstrapping 自助法。使用动态规划或者TD方法，而不是使用完整轨迹的reward。</li> <li>Off-policy training 离策略的训练方式。训练用的采样分布不是由目标策略产生的，而且一直在变化。</li> </ol> </article> </div> </div> <a href=# class="md-top md-icon" data-md-component=top data-md-state=hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12Z"/></svg> Back to top </a> </main> <footer class=md-footer> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs </a> </div> <div class=md-social> <a href=https://github.com/yangyichu target=_blank rel=noopener title=github.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><!-- Font Awesome Free 6.1.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg> </a> <a href=https://pypi.org/project/mkdocs-material/ target=_blank rel=noopener title=pypi.org class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.1.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M439.8 200.5c-7.7-30.9-22.3-54.2-53.4-54.2h-40.1v47.4c0 36.8-31.2 67.8-66.8 67.8H172.7c-29.2 0-53.4 25-53.4 54.3v101.8c0 29 25.2 46 53.4 54.3 33.8 9.9 66.3 11.7 106.8 0 26.9-7.8 53.4-23.5 53.4-54.3v-40.7H226.2v-13.6h160.2c31.1 0 42.6-21.7 53.4-54.2 11.2-33.5 10.7-65.7 0-108.6zM286.2 404c11.1 0 20.1 9.1 20.1 20.3 0 11.3-9 20.4-20.1 20.4-11 0-20.1-9.2-20.1-20.4.1-11.3 9.1-20.3 20.1-20.3zM167.8 248.1h106.8c29.7 0 53.4-24.5 53.4-54.3V91.9c0-29-24.4-50.7-53.4-55.6-35.8-5.9-74.7-5.6-106.8.1-45.2 8-53.4 24.7-53.4 55.6v40.7h106.9v13.6h-147c-31.1 0-58.3 18.7-66.8 54.2-9.8 40.7-10.2 66.1 0 108.6 7.6 31.6 25.7 54.2 56.8 54.2H101v-48.8c0-35.3 30.5-66.4 66.8-66.4zm-6.7-142.6c-11.1 0-20.1-9.1-20.1-20.3.1-11.3 9-20.4 20.1-20.4 11 0 20.1 9.2 20.1 20.4s-9 20.3-20.1 20.3z"/></svg> </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <script id=__config type=application/json>{"base": "../..", "features": ["content.code.annotate", "navigation.expand", "navigation.indexes", "navigation.tabs", "navigation.tabs.sticky", "navigation.top", "navigation.tracking", "search.highlight", "search.share", "search.suggest", "toc.follow"], "search": "../../assets/javascripts/workers/search.2a1c317c.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version.title": "Select version"}}</script> <script src=../../assets/javascripts/bundle.ed9748b7.min.js></script> <script src=../../javascripts/mathjax.js></script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src=https://cdn.bootcdn.net/ajax/libs/mathjax/3.2.0/es5/tex-chtml-full.min.js></script> </body> </html>