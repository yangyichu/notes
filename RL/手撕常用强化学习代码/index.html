<!doctype html><html lang=en class=no-js> <head><link rel=stylesheet href=assets/javascripts/main.bec4e691.min.css><link rel=stylesheet href=assets/javascripts/palette.96b5c56e.min.css><link rel=stylesheet href=assets/javascripts/main.9e77ca7e.min.css><link rel=stylesheet href=assets/javascripts/home.def3a648.min.css><link rel=stylesheet href=assets/javascripts/css><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=author content="Yichu Yang"><link href=https://yichuyang.com/notes/RL/%E6%89%8B%E6%92%95%E5%B8%B8%E7%94%A8%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E4%BB%A3%E7%A0%81/ rel=canonical><link rel=icon href=../../assets/favicon.png><meta name=generator content="mkdocs-1.3.0, mkdocs-material-8.2.13"><title>手撕常用强化学习代码 - Yichu's blog</title><link rel=stylesheet href=../../assets/stylesheets/main.e411adfe.min.css><link rel=stylesheet href=../../assets/stylesheets/palette.cc9b2e1e.min.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><link rel=stylesheet href=../../stylesheets/extra.css><script>__md_scope=new URL("../..",location),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo> <script>var palette=__md_get("__palette");if(palette&&"object"==typeof palette.color)for(var key of Object.keys(palette.color))document.body.setAttribute("data-md-color-"+key,palette.color[key])</script> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#1-dqn class=md-skip> Skip to content </a> </div> <div data-md-component=announce> </div> <header class="md-header md-header--lifted" data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=../.. title="Yichu's blog" class="md-header__button md-logo" aria-label="Yichu's blog" data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 89 89"> <path d=M3.136,17.387l0,42.932l42.932,21.467l-42.932,-64.399Z /> <path d=M21.91,8l42.933,64.398l-18.775,9.388l-42.932,-64.399l18.774,-9.387Z style="fill-opacity: 0.5"/> <path d=M67.535,17.387l-27.262,18.156l21.878,32.818l5.384,2.691l0,-53.665Z /> <path d=M67.535,17.387l0,53.666l18.774,-9.388l0,-53.665l-18.774,9.387Z style="fill-opacity: 0.25"/> </svg> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> Yichu's blog </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> 手撕常用强化学习代码 </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo aria-label="Switch to dark mode" type=radio name=__palette id=__palette_1> <label class="md-header__button md-icon" title="Switch to dark mode" for=__palette_2 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5m0 8a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3Z"/></svg> </label> <input class=md-option data-md-color-media data-md-color-scheme=slate data-md-color-primary=white data-md-color-accent=cyan aria-label="Switch to light mode" type=radio name=__palette id=__palette_2> <label class="md-header__button md-icon" title="Switch to light mode" for=__palette_1 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M17 6H7c-3.31 0-6 2.69-6 6s2.69 6 6 6h10c3.31 0 6-2.69 6-6s-2.69-6-6-6zm0 10H7c-2.21 0-4-1.79-4-4s1.79-4 4-4h10c2.21 0 4 1.79 4 4s-1.79 4-4 4zM7 9c-1.66 0-3 1.34-3 3s1.34 3 3 3 3-1.34 3-3-1.34-3-3-3z"/></svg> </label> </form> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg> </label> <nav class=md-search__options aria-label=Search> <a href=javascript:void(0) class="md-search__icon md-icon" aria-label=Share data-clipboard data-clipboard-text data-md-component=search-share tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7 0-.24-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91 1.61 0 2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08Z"/></svg> </a> <button type=reset class="md-search__icon md-icon" aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list></ol> </div> </div> </div> </div> </div> </nav> <nav class=md-tabs aria-label=Tabs data-md-component=tabs> <div class="md-tabs__inner md-grid"> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=../.. class=md-tabs__link> Home </a> </li> <li class=md-tabs__item> <a href=../../Sensor-Fusion/LOAM/ class=md-tabs__link> Sensor Fusion </a> </li> <li class=md-tabs__item> <a href=../../Motion-Planning/BCH-approximation/ class=md-tabs__link> Motion Planning </a> </li> <li class=md-tabs__item> <a href=./ class="md-tabs__link md-tabs__link--active"> 强化学习 </a> </li> <li class=md-tabs__item> <a href=https://yichuyang.com class=md-tabs__link> Resume </a> </li> </ul> </div> </nav> </header> <div class=md-container data-md-component=container> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=../.. title="Yichu's blog" class="md-nav__button md-logo" aria-label="Yichu's blog" data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 89 89"> <path d=M3.136,17.387l0,42.932l42.932,21.467l-42.932,-64.399Z /> <path d=M21.91,8l42.933,64.398l-18.775,9.388l-42.932,-64.399l18.774,-9.387Z style="fill-opacity: 0.5"/> <path d=M67.535,17.387l-27.262,18.156l21.878,32.818l5.384,2.691l0,-53.665Z /> <path d=M67.535,17.387l0,53.666l18.774,-9.388l0,-53.665l-18.774,9.387Z style="fill-opacity: 0.25"/> </svg> </a> Yichu's blog </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../.. class=md-nav__link> Home </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_2 data-md-state=indeterminate type=checkbox id=__nav_2 checked> <label class=md-nav__link for=__nav_2> Sensor Fusion <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Sensor Fusion" data-md-level=1> <label class=md-nav__title for=__nav_2> <span class="md-nav__icon md-icon"></span> Sensor Fusion </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../Sensor-Fusion/LOAM/ class=md-nav__link> LOAM </a> </li> <li class=md-nav__item> <a href=../../Sensor-Fusion/Ceres-Tutorial/ class=md-nav__link> Ceres 使用入门 </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_2_3 data-md-state=indeterminate type=checkbox id=__nav_2_3 checked> <label class=md-nav__link for=__nav_2_3> 惯性导航 <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=惯性导航 data-md-level=2> <label class=md-nav__title for=__nav_2_3> <span class="md-nav__icon md-icon"></span> 惯性导航 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../Sensor-Fusion/%E5%9B%9B%E5%85%83%E6%95%B0%E5%9F%BA%E7%A1%80%E5%92%8C%E6%83%AF%E6%80%A7%E5%AF%BC%E8%88%AA%E8%A7%A3%E7%AE%97/ class=md-nav__link> 四元数基础和惯性导航解算 </a> </li> <li class=md-nav__item> <a href=../../Sensor-Fusion/%E6%83%AF%E6%80%A7%E5%AF%BC%E8%88%AA%E5%8E%9F%E7%90%86%E5%8F%8A%E8%AF%AF%E5%B7%AE%E5%88%86%E6%9E%90/ class=md-nav__link> 惯性导航原理及误差分析 </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_2_4 data-md-state=indeterminate type=checkbox id=__nav_2_4 checked> <label class=md-nav__link for=__nav_2_4> 卡尔曼滤波 <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=卡尔曼滤波 data-md-level=2> <label class=md-nav__title for=__nav_2_4> <span class="md-nav__icon md-icon"></span> 卡尔曼滤波 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../Sensor-Fusion/%E5%8D%A1%E5%B0%94%E6%9B%BC%E6%BB%A4%E6%B3%A2/ class=md-nav__link> 卡尔曼滤波 </a> </li> <li class=md-nav__item> <a href=../../Sensor-Fusion/%E5%8D%A1%E5%B0%94%E6%9B%BC%E6%BB%A4%E6%B3%A2%E6%8E%A8%E5%AF%BC/ class=md-nav__link> 卡尔曼滤波推导 </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_2_5 data-md-state=indeterminate type=checkbox id=__nav_2_5 checked> <label class=md-nav__link for=__nav_2_5> 误差状态卡尔曼(ESKF)笔记 <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=误差状态卡尔曼(ESKF)笔记 data-md-level=2> <label class=md-nav__title for=__nav_2_5> <span class="md-nav__icon md-icon"></span> 误差状态卡尔曼(ESKF)笔记 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../Sensor-Fusion/ESKF%E5%8A%A8%E5%8A%9B%E5%AD%A6%E6%96%B9%E7%A8%8B/ class=md-nav__link> ESKF动力学方程 </a> </li> <li class=md-nav__item> <a href=../../Sensor-Fusion/ESKF%E7%A6%BB%E6%95%A3%E5%8A%A8%E5%8A%9B%E5%AD%A6%E6%96%B9%E7%A8%8B/ class=md-nav__link> ESKF离散动力学方程 </a> </li> <li class=md-nav__item> <a href=../../Sensor-Fusion/ESKF%E8%AF%AF%E5%B7%AE%E7%8A%B6%E6%80%81%E6%BB%A4%E6%B3%A2%E5%99%A8/ class=md-nav__link> ESKF误差状态滤波器 </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_2_6 data-md-state=indeterminate type=checkbox id=__nav_2_6 checked> <label class=md-nav__link for=__nav_2_6> 图优化 <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=图优化 data-md-level=2> <label class=md-nav__title for=__nav_2_6> <span class="md-nav__icon md-icon"></span> 图优化 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../Sensor-Fusion/%E9%A2%84%E7%A7%AF%E5%88%86%E6%8E%A8%E5%AF%BC/ class=md-nav__link> 预积分推导 </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_3 data-md-state=indeterminate type=checkbox id=__nav_3 checked> <label class=md-nav__link for=__nav_3> Motion Planning <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Motion Planning" data-md-level=1> <label class=md-nav__title for=__nav_3> <span class="md-nav__icon md-icon"></span> Motion Planning </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../Motion-Planning/BCH-approximation/ class=md-nav__link> BCH approximation </a> </li> <li class=md-nav__item> <a href=../../Motion-Planning/ICP-realization/ class=md-nav__link> ICP realization </a> </li> <li class=md-nav__item> <a href=../../Motion-Planning/matrix_diff/ class=md-nav__link> Matrix diff </a> </li> <li class=md-nav__item> <a href=../../Motion-Planning/trajectory-optimization/ class=md-nav__link> Trajectory optimization </a> </li> <li class=md-nav__item> <a href=../../Motion-Planning/Polynomial-Trajectory-Planning-for-Aggressive-Quadrotor-Flight/ class=md-nav__link> Polynomial Trajectory Planning for Aggressive Quadrotor Flight </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_3_6 data-md-state=indeterminate type=checkbox id=__nav_3_6 checked> <label class=md-nav__link for=__nav_3_6> Fast/Ego-Planner <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=Fast/Ego-Planner data-md-level=2> <label class=md-nav__title for=__nav_3_6> <span class="md-nav__icon md-icon"></span> Fast/Ego-Planner </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_3_6_1 data-md-state=indeterminate type=checkbox id=__nav_3_6_1 checked> <label class=md-nav__link for=__nav_3_6_1> 地图形式 <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=地图形式 data-md-level=3> <label class=md-nav__title for=__nav_3_6_1> <span class="md-nav__icon md-icon"></span> 地图形式 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../Motion-Planning/Occupancy-Grid-Map/ class=md-nav__link> Occupancy Grid Map </a> </li> <li class=md-nav__item> <a href=../../Motion-Planning/ESDF/ class=md-nav__link> ESDF </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../../Motion-Planning/B-spline/ class=md-nav__link> B spline </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--active md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_4 type=checkbox id=__nav_4 checked> <label class=md-nav__link for=__nav_4> 强化学习 <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=强化学习 data-md-level=1> <label class=md-nav__title for=__nav_4> <span class="md-nav__icon md-icon"></span> 强化学习 </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" data-md-toggle=toc type=checkbox id=__toc> <label class="md-nav__link md-nav__link--active" for=__toc> 手撕常用强化学习代码 <span class="md-nav__icon md-icon"></span> </label> <a href=./ class="md-nav__link md-nav__link--active"> 手撕常用强化学习代码 </a> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#1-dqn class=md-nav__link> 1. DQN </a> <nav class=md-nav aria-label="1. DQN"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#11-double-dqn class=md-nav__link> 1.1 Double DQN </a> </li> <li class=md-nav__item> <a href=#12-dueling-dqn class=md-nav__link> 1.2 Dueling DQN </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#2-policy-gradient class=md-nav__link> 2. Policy Gradient </a> <nav class=md-nav aria-label="2. Policy Gradient"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#21-vanilla-pg class=md-nav__link> 2.1 Vanilla PG </a> </li> <li class=md-nav__item> <a href=#22-trpo class=md-nav__link> 2.2 TRPO </a> </li> <li class=md-nav__item> <a href=#23-ppoclipped class=md-nav__link> 2.3 PPO(clipped) </a> </li> <li class=md-nav__item> <a href=#24-ddpg class=md-nav__link> 2.4 DDPG </a> <nav class=md-nav aria-label="2.4 DDPG"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#241-the-q-learning-side-of-ddpg class=md-nav__link> 2.4.1 The Q-Learning Side of DDPG </a> </li> <li class=md-nav__item> <a href=#242-the-policy-learning-side-of-ddpg class=md-nav__link> 2.4.2 The Policy Learning Side of DDPG </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#25-td3 class=md-nav__link> 2.5 TD3 </a> </li> <li class=md-nav__item> <a href=#26-sac class=md-nav__link> 2.6 SAC </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../DRQN%E5%AE%9E%E7%8E%B0%E7%9A%84%E4%B8%80%E4%BA%9B%E7%BB%86%E8%8A%82/ class=md-nav__link> DRQN实现的一些细节 </a> </li> <li class=md-nav__item> <a href=../R2D2-%E7%AC%94%E8%AE%B0/ class=md-nav__link> R2D2-笔记 </a> </li> <li class=md-nav__item> <a href=../Diffusion-Model/ class=md-nav__link> Diffusion Model </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_5 data-md-state=indeterminate type=checkbox id=__nav_5 checked> <label class=md-nav__link for=__nav_5> Resume <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=Resume data-md-level=1> <label class=md-nav__title for=__nav_5> <span class="md-nav__icon md-icon"></span> Resume </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=https://yichuyang.com class=md-nav__link> None </a> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#1-dqn class=md-nav__link> 1. DQN </a> <nav class=md-nav aria-label="1. DQN"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#11-double-dqn class=md-nav__link> 1.1 Double DQN </a> </li> <li class=md-nav__item> <a href=#12-dueling-dqn class=md-nav__link> 1.2 Dueling DQN </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#2-policy-gradient class=md-nav__link> 2. Policy Gradient </a> <nav class=md-nav aria-label="2. Policy Gradient"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#21-vanilla-pg class=md-nav__link> 2.1 Vanilla PG </a> </li> <li class=md-nav__item> <a href=#22-trpo class=md-nav__link> 2.2 TRPO </a> </li> <li class=md-nav__item> <a href=#23-ppoclipped class=md-nav__link> 2.3 PPO(clipped) </a> </li> <li class=md-nav__item> <a href=#24-ddpg class=md-nav__link> 2.4 DDPG </a> <nav class=md-nav aria-label="2.4 DDPG"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#241-the-q-learning-side-of-ddpg class=md-nav__link> 2.4.1 The Q-Learning Side of DDPG </a> </li> <li class=md-nav__item> <a href=#242-the-policy-learning-side-of-ddpg class=md-nav__link> 2.4.2 The Policy Learning Side of DDPG </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#25-td3 class=md-nav__link> 2.5 TD3 </a> </li> <li class=md-nav__item> <a href=#26-sac class=md-nav__link> 2.6 SAC </a> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <h1>手撕常用强化学习代码</h1> <h2 id=1-dqn>1. <strong>DQN</strong><a class=headerlink href=#1-dqn title="Permanent link">¶</a></h2> <p><a href=https://arxiv.org/abs/1312.5602><em>https://arxiv.org/abs/1312.5602</em></a></p> <!-- ![image-20211127235059145](./dqn.jpg) --> <p>损失函数为MSE：</p> <div class=arithmatex>\[ L(\omega)=\mathbf{E}[(\underbrace{r+\gamma max_{a'}Q(s',a',\omega)}_{Target}-Q(s,a,\omega))^2] \]</div> <p>梯度： <span class=arithmatex>\(r+\gamma max_{a'}Q(s',a',\omega)-Q(s,a,\omega)\)</span></p> <p>算法：</p> <!-- ![image-20211127235059145](./dqn-target.jpg) --> <p>Bellman equation describing the optimal action-value function, <img alt=x src=https://spinningup.openai.com/en/latest/_images/math/cbed396f671d6fb54f6df5c044b82ab3f052d63e.svg>. </p> <p>It’s given by</p> <p><img alt=x src=https://spinningup.openai.com/en/latest/_images/math/3a8b6ce0d6c0b68744b5724403f5d70ed5cda5db.svg></p> <p><strong>mean-squared Bellman error (MSBE)</strong> function, which tells us roughly how closely <img alt=x src=https://spinningup.openai.com/en/latest/_images/math/c25464faf1bf4928960905461cbbabe1d2441cb2.svg> comes to satisfying the Bellman equation:</p> <p><img alt=x src=https://spinningup.openai.com/en/latest/_images/math/31dda6ac0678255c4e192dd6fae4f7ed3c7cd91b.svg></p> <p>主要改进： </p> <ol> <li> <p>Experience Replay: 储存加随机采样回放，降低样本间相关性，降低方差</p> </li> <li> <p>Target Network: <span class=arithmatex>\(L=(r-\gamma Q_{target}-Q_{behavior})^2\)</span></p> </li> </ol> <!--more--> <p><img alt=image-20211128163448569 src=https://yichu-1305057184.cos.ap-shanghai.myqcloud.com/typora/image-20211128163448569.png></p> <p><img alt=image-20211128163513253 src=https://yichu-1305057184.cos.ap-shanghai.myqcloud.com/typora/image-20211128163513253.png></p> <p><img alt=image-20211128163245890 src=https://yichu-1305057184.cos.ap-shanghai.myqcloud.com/typora/image-20211128163245890.png></p> <h3 id=11-double-dqn>1.1 Double DQN<a class=headerlink href=#11-double-dqn title="Permanent link">¶</a></h3> <p><img alt=image-20211128163258450 src=C:\Users\yichu\AppData\Roaming\Typora\typora-user-images\image-20211128163258450.png></p> <h3 id=12-dueling-dqn>1.2 Dueling DQN<a class=headerlink href=#12-dueling-dqn title="Permanent link">¶</a></h3> <p><img alt=image-20211128163306443 src=C:\Users\yichu\AppData\Roaming\Typora\typora-user-images\image-20211128163306443.png></p> <h2 id=2-policy-gradient>2. Policy Gradient<a class=headerlink href=#2-policy-gradient title="Permanent link">¶</a></h2> <h3 id=21-vanilla-pg>2.1 Vanilla PG<a class=headerlink href=#21-vanilla-pg title="Permanent link">¶</a></h3> <p>VPG trains a stochastic policy in an on-policy way. This means that it explores by sampling actions according to the latest version of its stochastic policy. This may cause the policy to get trapped in local optima.</p> <p><img alt=x src=https://spinningup.openai.com/en/latest/_images/math/ada1266646d71c941e77e3fd41bba9d92d06b7c2.svg></p> <p>Advantage Function: A=V-R</p> <!-- ![image-20211128004331312](./pg.svg) --> <p>from <a href=https://spinningup.openai.com/en/latest/algorithms/vpg.html#pseudocode>spinningup</a>.</p> <h3 id=22-trpo>2.2 TRPO<a class=headerlink href=#22-trpo title="Permanent link">¶</a></h3> <p>Let <img alt=x src=https://spinningup.openai.com/en/latest/_images/math/6a71f04b65d9524fb656715cda85d7540a9ddf9f.svg> denote a policy with parameters <img alt=x src=https://spinningup.openai.com/en/latest/_images/math/ce5edddd490112350f4bd555d9390e0e845f754a.svg>. The theoretical TRPO update is:</p> <p><img alt=x src=https://spinningup.openai.com/en/latest/_images/math/23edf1f72f63a4729c40371c1481a36549a0b713.svg></p> <p>where <img alt=x src=https://spinningup.openai.com/en/latest/_images/math/0837b005b194415b2b922e42be1df8601b552857.svg> is the <em>surrogate advantage</em>, a measure of how policy <img alt=x src=https://spinningup.openai.com/en/latest/_images/math/6a71f04b65d9524fb656715cda85d7540a9ddf9f.svg> performs relative to the old policy <img alt=x src=https://spinningup.openai.com/en/latest/_images/math/d8bb9f337fa712549e0428223df820773aa1169d.svg> using data from the old policy:</p> <p><img alt=x src=https://spinningup.openai.com/en/latest/_images/math/ae8edab1e9c727bed15e54d4dda492382538b5fe.svg></p> <p>and <img alt=x src=https://spinningup.openai.com/en/latest/_images/math/88396050a58384b85dfaa6fce02cf39d98c78c4b.svg> is an average KL-divergence between policies across states visited by the old policy:</p> <p><img alt=x src=https://spinningup.openai.com/en/latest/_images/math/78a651e0ce4979bd3e17198594ad952ac20b9b45.svg></p> <p>We Taylor expand the objective and constraint to leading order around <img alt=x src=https://spinningup.openai.com/en/latest/_images/math/a485f77ef16acbb27539cdfe8286cd6029ccfd26.svg>:</p> <p><img alt={x src=https://spinningup.openai.com/en/latest/_images/math/7cdaa039734ec1d09adcc3e4dc351085823085cf.svg></p> <p>resulting in an approximate optimization problem,</p> <p><img alt=x src=https://spinningup.openai.com/en/latest/_images/math/69c9dcbe2fe1c669a1b2cb3a312a479cdfcb27a1.svg></p> <p>This approximate problem can be analytically solved by the methods of Lagrangian duality [x]](<a href=https://spinningup.openai.com/en/latest/algorithms/trpo.html#id2>https://spinningup.openai.com/en/latest/algorithms/trpo.html#id2</a>), yielding the solution:</p> <p><img alt=x src=https://spinningup.openai.com/en/latest/_images/math/e990f7ff0230a8fa93cf1242ea0d49fdf63d05d7.svg></p> <p>If we were to stop here, and just use this final result, the algorithm would be exactly calculating the <a href=https://papers.nips.cc/paper/2073-a-natural-policy-gradient.pdf>Natural Policy Gradient</a>. TRPO adds a modification to this update rule: a backtracking line search,</p> <p><img alt=x src=https://spinningup.openai.com/en/latest/_images/math/03cabd66ab79d8c17e36fc4247bb46fe0c6dcbfc.svg><img alt=image-20211128005202175 src=https://yichu-1305057184.cos.ap-shanghai.myqcloud.com/typora/image-20211128005202175.png></p> <h3 id=23-ppoclipped>2.3 PPO(clipped)<a class=headerlink href=#23-ppoclipped title="Permanent link">¶</a></h3> <p><img alt=x src=https://spinningup.openai.com/en/latest/_images/math/96a52e61318720522e040e433c938ee829d54506.svg></p> <p>typically taking multiple steps of (usually minibatch) SGD to maximize the objective. Here <img alt=L src=https://spinningup.openai.com/en/latest/_images/math/3ffe1da701d78dd473975ebd2f875807611f7713.svg> is given by</p> <p><img alt=x src=https://spinningup.openai.com/en/latest/_images/math/99621d5bcaccd056d6ca3aeb48a27bf8cc0e640c.svg></p> <p>simpler version: <img alt=x src=https://spinningup.openai.com/en/latest/_images/math/dd41a29292af3bc58c0c76bc7dba82a7355bf929.svg></p> <p>where: <img alt=x src=https://spinningup.openai.com/en/latest/_images/math/39f524858866b80e627840ba77a54360e3bac55e.svg><em>the new policy does not benefit by going far away from the old policy</em></p> <p><img alt=x src=https://yichu-1305057184.cos.ap-shanghai.myqcloud.com/typora/image-20211128010316710.png></p> <h3 id=24-ddpg>2.4 DDPG<a class=headerlink href=#24-ddpg title="Permanent link">¶</a></h3> <h4 id=241-the-q-learning-side-of-ddpg>2.4.1 <a href=https://spinningup.openai.com/en/latest/algorithms/ddpg.html#id5>The Q-Learning Side of DDPG</a><a class=headerlink href=#241-the-q-learning-side-of-ddpg title="Permanent link">¶</a></h4> <p>polyak averaging:</p> <p><img alt=x src=https://spinningup.openai.com/en/latest/_images/math/d417987803ca9f61ac60741880a748129bd66dde.svg></p> <p>DDPG deals with computing the maximum over actions in the target in <strong>continuous action spaces</strong> by using a <strong>target policy network</strong> to compute an action which approximately maximizes <img alt src=https://spinningup.openai.com/en/latest/_images/math/a50d5d2b71fa30f115adf18b0bb1354f967b064a.svg>.</p> <p><img alt=x src=https://spinningup.openai.com/en/latest/_images/math/4421120861d55302d76c7e2fd7cc5b2da7aea320.svg></p> <p>where <img alt src=https://spinningup.openai.com/en/latest/_images/math/a325c9e05fa2ccce85eb2384ca00b4888d1c7824.svg> is the target policy.</p> <h4 id=242-the-policy-learning-side-of-ddpg>2.4.2 <a href=https://spinningup.openai.com/en/latest/algorithms/ddpg.html#id6>The Policy Learning Side of DDPG</a><a class=headerlink href=#242-the-policy-learning-side-of-ddpg title="Permanent link">¶</a></h4> <p>We want to learn a deterministic policy <img alt src=https://spinningup.openai.com/en/latest/_images/math/6923cb2043e84ea05d3eddbb7436c60659243cb9.svg> which gives the action that maximizes <img alt src=https://spinningup.openai.com/en/latest/_images/math/521198ffdba43bf32186f95801549cd1502b76c7.svg>.</p> <p>perform gradient ascent (with respect to policy parameters only) to solve: <img alt=x src=https://spinningup.openai.com/en/latest/_images/math/cc4e3565d839e63e871a1cf7e3ce5e95bb616b29.svg></p> <p>Note that the Q-function parameters are treated as constants here.</p> <p>DDPG trains a deterministic policy in an off-policy way. Because the policy is deterministic, if the agent were to explore on-policy, in the beginning it would probably not try a wide enough variety of actions to find useful learning signals. To make DDPG policies explore better, we add noise to their actions at training time.</p> <p><img alt=image-20211128015259509 src=https://yichu-1305057184.cos.ap-shanghai.myqcloud.com/typora/image-20211128015259509.png></p> <h3 id=25-td3>2.5 TD3<a class=headerlink href=#25-td3 title="Permanent link">¶</a></h3> <p>A common failure mode for DDPG is that the learned Q-function begins to dramatically overestimate Q-values, which then leads to the policy breaking, because it exploits the errors in the Q-function. </p> <p><strong>Trick One: Clipped Double-Q Learning.</strong> TD3 learns <em>two</em> Q-functions instead of one (hence “twin”), and uses the smaller of the two Q-values to form the targets in the Bellman error loss functions.</p> <p><strong>Trick Two: “Delayed” Policy Updates.</strong> TD3 updates the policy (and target networks) less frequently than the Q-function. The paper recommends one policy update for every two Q-function updates.</p> <p><strong>Trick Three: Target Policy Smoothing.</strong> TD3 adds noise to the target action, to make it harder for the policy to exploit Q-function errors by smoothing out Q along changes in action.</p> <p><strong>target policy smoothing</strong>. Actions used to form the Q-learning target are based on the target policy, <img alt=x src=https://spinningup.openai.com/en/latest/_images/math/a325c9e05fa2ccce85eb2384ca00b4888d1c7824.svg>, but with clipped noise added on each dimension of the action. After adding the clipped noise, the target action is then clipped to lie in the valid action range (all valid actions, <img alt=a src=https://spinningup.openai.com/en/latest/_images/math/76a319586cd215c8f2075b938fc6f6e07c81714b.svg>, satisfy <img alt src=https://spinningup.openai.com/en/latest/_images/math/a5132668c0af8733656505c5fb6c1dff4a7907a1.svg>). The target actions are thus:</p> <p><img alt=x src=https://spinningup.openai.com/en/latest/_images/math/8efd61c40551db4eddb3f780d2804cac34c8ae52.svg></p> <p><strong>clipped double-Q learning</strong>. Both Q-functions use a single target, calculated using whichever of the two Q-functions gives a smaller target value:</p> <p><img alt src=https://spinningup.openai.com/en/latest/_images/math/70901eaea34c31e03bb878d7a710a33cb75d1143.svg></p> <p>and then both are learned by regressing to this target:</p> <p><img alt src=https://spinningup.openai.com/en/latest/_images/math/7d5c18f49a242cc3eec554f717fe4f3bfc119bab.svg></p> <p><img alt src=https://spinningup.openai.com/en/latest/_images/math/cd73726a8a3845ade467aed57714912f868f6b36.svg></p> <p>Using the smaller Q-value for the target, and regressing towards that, helps fend off overestimation in the Q-function.</p> <p>Lastly: the policy is learned just by maximizing <img alt src=https://spinningup.openai.com/en/latest/_images/math/8795d42bd263dcbe55d123e7466b2dd5091490a7.svg>:</p> <p><img alt src=https://spinningup.openai.com/en/latest/_images/math/9ed1a541005a48d51b624c3b329897064ec2c065.svg></p> <p><img alt=image-20211128020516543 src=https://yichu-1305057184.cos.ap-shanghai.myqcloud.com/typora/image-20211128020516543.png></p> <h3 id=26-sac>2.6 SAC<a class=headerlink href=#26-sac title="Permanent link">¶</a></h3> <p>A central feature of SAC is <strong>entropy regularization.</strong> The policy is trained to maximize a trade-off between expected return and <a href=https://en.wikipedia.org/wiki/Entropy_(information_theory)>entropy</a>, a measure of randomness in the policy.</p> <p><a href=https://spinningup.openai.com/en/latest/algorithms/sac.html#id6>Entropy-Regularized Reinforcement Learning</a></p> <p>Let <img alt=x src=https://spinningup.openai.com/en/latest/_images/math/ea07a4204f1f53321f76d9c7e348199f0d707db1.svg> be a random variable with probability mass or density function <img alt=P src=https://spinningup.openai.com/en/latest/_images/math/4204ba416334e663d7bd7c6457d737ba3cbbfe46.svg>. The entropy <img alt=H src=https://spinningup.openai.com/en/latest/_images/math/bf6bcb1745aeab36cdc185e9f75bbfd3998352ce.svg> of <img alt=x src=https://spinningup.openai.com/en/latest/_images/math/ea07a4204f1f53321f76d9c7e348199f0d707db1.svg> is computed from its distribution <img alt=P src=https://spinningup.openai.com/en/latest/_images/math/4204ba416334e663d7bd7c6457d737ba3cbbfe46.svg> according to</p> <p><img alt src=https://spinningup.openai.com/en/latest/_images/math/1bf89d5228652e14d82657fe9f1499b136f54094.svg></p> <p>In entropy-regularized reinforcement learning, the agent gets a bonus reward at each time step proportional to the entropy of the policy at that timestep. This changes <a href=https://spinningup.openai.com/en/latest/spinningup/rl_intro.html#the-rl-problem>the RL problem</a> to:</p> <p><img alt src=https://spinningup.openai.com/en/latest/_images/math/b86bf499707114c8789946df649871c5b9185b9d.svg></p> <p>With these definitions, <img alt src=https://spinningup.openai.com/en/latest/_images/math/fbed8ae629f7512710c5352ca50e8f629d7f34e4.svg> and <img alt src=https://spinningup.openai.com/en/latest/_images/math/2bbd8ab5668fe92f59056f58c9f75a01c929e37d.svg> are connected by:</p> <p><img alt src=https://spinningup.openai.com/en/latest/_images/math/46d0852616c131f3d5aa2d1798328141904a764d.svg></p> <p>and the Bellman equation for <img alt src=https://spinningup.openai.com/en/latest/_images/math/2bbd8ab5668fe92f59056f58c9f75a01c929e37d.svg> is</p> <p><img alt src=https://spinningup.openai.com/en/latest/_images/math/8010672f1e8269ce985f901728e7224faa07731e.svg></p> <p>the loss functions for the Q-networks in SAC are:</p> <p><img alt src=https://spinningup.openai.com/en/latest/_images/math/0bd81fc5d1cb03a33d6477f5ff10ed879ea393ec.svg></p> <p>where the target is given by</p> <p><img alt src=https://spinningup.openai.com/en/latest/_images/math/fc03ff9e9f818fb31b7724907e2b43d5101d2ab8.svg></p> <p><strong>Learning the Policy.</strong> The policy should, in each state, act to maximize the expected future return plus expected future entropy. That is, it should maximize <img alt src=https://spinningup.openai.com/en/latest/_images/math/a81303323c25fc13cd0652ca46d7596276e5cb7e.svg>, which we expand out into</p> <p><img alt src=https://spinningup.openai.com/en/latest/_images/math/5ff58df73caef07f6309a1460fe57b1c34e3b374.svg></p> <p>The way we optimize the policy makes use of the <strong>reparameterization trick</strong>, in which a sample from <img alt src=https://spinningup.openai.com/en/latest/_images/math/e57f13375048b8f7343f9066b6553bc282afa326.svg> is drawn by computing a deterministic function of state, policy parameters, and independent noise. To illustrate: following the authors of the SAC paper, we use a squashed Gaussian policy, which means that samples are obtained according to</p> <p><img alt src=https://spinningup.openai.com/en/latest/_images/math/dac3ddc2ea35e8233b8bc0a905273712793ab1cb.svg></p> <p>The reparameterization trick allows us to rewrite the expectation over actions (which contains a pain point: the distribution depends on the policy parameters) into an expectation over noise (which removes the pain point: the distribution now has no dependence on parameters):</p> <p><img alt src=https://spinningup.openai.com/en/latest/_images/math/5713f9f99ea3532e3cbde89eac91328eb8549409.svg></p> <p>To get the policy loss, the final step is that we need to substitute <img alt src=https://spinningup.openai.com/en/latest/_images/math/9c39112fd52e66e3062f93c502ade0eb9381d957.svg> with one of our function approximators. Unlike in TD3, which uses <img alt src=https://spinningup.openai.com/en/latest/_images/math/8795d42bd263dcbe55d123e7466b2dd5091490a7.svg> (just the first Q approximator), SAC uses <img alt src=https://spinningup.openai.com/en/latest/_images/math/e5d14ed1b7128d64d43af73b7d0b189c6afda8ec.svg> (the minimum of the two Q approximators). The policy is thus optimized according to</p> <p><img alt src=https://spinningup.openai.com/en/latest/_images/math/bdbe4cabbba4687b310d99e8fa67ed314339bd31.svg></p> <p>which is almost the same as the DDPG and TD3 policy optimization, except for the min-double-Q trick, the stochasticity, and the entropy term.</p> <p><img alt=image-20211128022803736 src=https://yichu-1305057184.cos.ap-shanghai.myqcloud.com/typora/image-20211128022803736.png></p> </article> </div> </div> <a href=# class="md-top md-icon" data-md-component=top data-md-state=hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12Z"/></svg> Back to top </a> </main> <footer class=md-footer> <nav class="md-footer__inner md-grid" aria-label=Footer> <a href=../../Motion-Planning/B-spline/ class="md-footer__link md-footer__link--prev" aria-label="Previous: B spline" rel=prev> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg> </div> <div class=md-footer__title> <div class=md-ellipsis> <span class=md-footer__direction> Previous </span> B spline </div> </div> </a> <a href=../DRQN%E5%AE%9E%E7%8E%B0%E7%9A%84%E4%B8%80%E4%BA%9B%E7%BB%86%E8%8A%82/ class="md-footer__link md-footer__link--next" aria-label="Next: DRQN实现的一些细节" rel=next> <div class=md-footer__title> <div class=md-ellipsis> <span class=md-footer__direction> Next </span> DRQN实现的一些细节 </div> </div> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4Z"/></svg> </div> </a> </nav> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs </a> </div> <div class=md-social> <a href=https://github.com/yangyichu target=_blank rel=noopener title=github.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><!-- Font Awesome Free 6.1.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg> </a> <a href=https://pypi.org/project/mkdocs-material/ target=_blank rel=noopener title=pypi.org class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.1.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M439.8 200.5c-7.7-30.9-22.3-54.2-53.4-54.2h-40.1v47.4c0 36.8-31.2 67.8-66.8 67.8H172.7c-29.2 0-53.4 25-53.4 54.3v101.8c0 29 25.2 46 53.4 54.3 33.8 9.9 66.3 11.7 106.8 0 26.9-7.8 53.4-23.5 53.4-54.3v-40.7H226.2v-13.6h160.2c31.1 0 42.6-21.7 53.4-54.2 11.2-33.5 10.7-65.7 0-108.6zM286.2 404c11.1 0 20.1 9.1 20.1 20.3 0 11.3-9 20.4-20.1 20.4-11 0-20.1-9.2-20.1-20.4.1-11.3 9.1-20.3 20.1-20.3zM167.8 248.1h106.8c29.7 0 53.4-24.5 53.4-54.3V91.9c0-29-24.4-50.7-53.4-55.6-35.8-5.9-74.7-5.6-106.8.1-45.2 8-53.4 24.7-53.4 55.6v40.7h106.9v13.6h-147c-31.1 0-58.3 18.7-66.8 54.2-9.8 40.7-10.2 66.1 0 108.6 7.6 31.6 25.7 54.2 56.8 54.2H101v-48.8c0-35.3 30.5-66.4 66.8-66.4zm-6.7-142.6c-11.1 0-20.1-9.1-20.1-20.3.1-11.3 9-20.4 20.1-20.4 11 0 20.1 9.2 20.1 20.4s-9 20.3-20.1 20.3z"/></svg> </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <script id=__config type=application/json>{"base": "../..", "features": ["content.code.annotate", "navigation.expand", "navigation.indexes", "navigation.tabs", "navigation.tabs.sticky", "navigation.top", "navigation.tracking", "search.highlight", "search.share", "search.suggest", "toc.follow"], "search": "../../assets/javascripts/workers/search.2a1c317c.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version.title": "Select version"}}</script> <script src=../../assets/javascripts/bundle.ed9748b7.min.js></script> <script src=../../javascripts/mathjax.js></script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src=https://cdn.bootcdn.net/ajax/libs/mathjax/3.2.0/es5/tex-chtml-full.min.js></script> </body> </html>